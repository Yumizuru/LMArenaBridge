# ğŸš€ LMArena Bridge - AI Model Arena API Proxy ğŸŒ‰

Welcome to the next generation of LMArena Bridge! ğŸ‰ This is a high-performance toolkit based on FastAPI and WebSocket, enabling seamless use of the vast language models available on the [LMArena.ai](https://lmarena.ai/) platform through any OpenAI API-compatible client or application.

This refactored version aims to provide a more stable, maintainable, and extensible experience.

## âœ¨ Key Features

*   **ğŸš€ High-Performance Backend**: Built on **FastAPI** and **Uvicorn**, offering asynchronous, high-performance API services.
*   **ğŸ”Œ Stable WebSocket Communication**: Replaces Server-Sent Events (SSE) with WebSocket for more reliable, low-latency bidirectional communication.
*   **ğŸ¤– OpenAI-Compatible Interface**: Fully compatible with OpenAI `v1/chat/completions`, `v1/models`, and `v1/images/generations` endpoints.
*   **ğŸ“‹ Manual Model List Updates**: New `model_updater.py` script allows manual extraction of the latest available model list from the LMArena page and saves it as `available_models.json` for easy reference and updates to the core `models.json`.
*   **ğŸ“ General File Upload**: Supports Base64 uploads of any file type (images, audio, PDFs, code, etc.) and allows multiple files to be uploaded at once.
*   **ğŸ¨ Native Streamed Image Generation**: Unified with text generation. Simply request an image model in the `/v1/chat/completions` endpoint to receive images in Markdown format, just like text.
*   **ğŸ—£ï¸ Full Conversation History Support**: Automatically injects conversation history into LMArena for context-aware continuous dialogue.
*   **ğŸŒŠ Real-Time Streamed Responses**: Receive text responses from models in real-time, just like the native OpenAI API.
*   **ğŸ”„ Automatic Program Updates**: Automatically checks the GitHub repository at startup and downloads and updates the program when a new version is found.
*   **ğŸ†” One-Click Session ID Updates**: Provides the `id_updater.py` script to automatically capture and update the session ID required in `config.jsonc` with a single browser operation.
*   **âš™ï¸ Browser Automation**: The accompanying Tampermonkey script (`LMArenaApiBridge.js`) handles communication with the backend server and performs all necessary operations in the browser.
*   **ğŸ» Tavern Mode**: Designed for applications like SillyTavern, intelligently merges `system` prompts to ensure compatibility.
*   **ğŸ¤« Bypass Mode**: Attempts to bypass platform-sensitive word checks by injecting an additional empty user message into the request.
*   **ğŸ” API Key Protection**: Set an API Key in the configuration file to add a layer of security to your service.
*   **ğŸ¯ Advanced Model-Session Mapping**: Supports configuring independent session ID pools for different models and specifying specific working modes (e.g., `battle` or `direct_chat`) for each session, enabling more precise request control.

## âš™ï¸ Configuration File Explanation

The main behavior of the project is controlled by `config.jsonc`, `models.json`, and `model_endpoint_map.json`.

### `models.json` - Core Model Mapping
This file contains the mapping of model names on the LMArena platform to their internal IDs and supports specifying model types in a specific format.

*   **Important**: This is the **essential** core file required for the program to run. You need to maintain this list manually.
*   **Format**:
    *   **Standard Text Models**: `"model-name": "model-id"`
    *   **Image Generation Models**: `"model-name": "model-id:image"`
*   **Explanation**:
    *   The program identifies image models by checking if the model ID string contains `:image`.
    *   This format maintains maximum compatibility with old configuration files; models without a specified type default to `"text"`.
*   **Example**:
    ```json
    {
      "gemini-1.5-pro-flash-20240514": "gemini-1.5-pro-flash-20240514",
      "dall-e-3": "null:image"
    }
    ```

### `available_models.json` - Reference for Available Models (Optional)
*   This is a **reference file** generated by the new `model_updater.py` script.
*   It contains complete information (ID, name, organization, etc.) about all models extracted from the LMArena page.
*   You can run `model_updater.py` to generate or update this file, then copy the model information you need into `models.json`.

### `config.jsonc` - Global Configuration

This is the main configuration file containing the global settings for the server.

*   `session_id` / `message_id`: Global default session ID. When a model is not found in `model_endpoint_map.json`, the ID here will be used.
*   `id_updater_last_mode` / `id_updater_battle_target`: Global default request mode. Similarly, when a specific session does not specify a mode, the settings here will be used.
*   `use_default_ids_if_mapping_not_found`: A very important toggle (default is `true`).
    *   `true`: If the requested model is not found in `model_endpoint_map.json`, the global default ID and mode will be used.
    *   `false`: If no mapping is found, an error will be returned. This is useful when you need strict control over each model's session.
*   Other configuration items such as `api_key`, `tavern_mode_enabled`, etc., please refer to the comments in the file.

### `model_endpoint_map.json` - Model-Specific Configuration

This is a powerful advanced feature that allows you to override global configurations and set one or more specific sessions for a particular model.

**Core Advantages**:
1.  **Session Isolation**: Use independent sessions for different models to avoid context interference.
2.  **Increase Concurrency**: Configure an ID pool for popular models. The program will randomly select one ID for each request, simulating polling and reducing the risk of a single session being frequently requested.
3.  **Mode Binding**: Bind a session ID to the mode (`direct_chat` or `battle`) when it was captured, ensuring the request format is always correct.

**Configuration Example**:
```json
{
  "claude-3-opus-20240229": [
    {
      "session_id": "session_for_direct_chat_1",
      "message_id": "message_for_direct_chat_1",
      "mode": "direct_chat"
    },
    {
      "session_id": "session_for_battle_A",
      "message_id": "message_for_battle_A",
      "mode": "battle",
      "battle_target": "A"
    }
  ],
  "gemini-1.5-pro-20241022": {
      "session_id": "single_session_id_no_mode",
      "message_id": "single_message_id_no_mode"
  }
}
```
*   **Opus**: Configured with an ID pool. Requests will randomly select one and strictly follow its bound `mode` and `battle_target`.
*   **Gemini**: Uses a single ID object (old format, still compatible). Since it does not specify `mode`, the program will automatically use the global mode defined in `config.jsonc`.

## ğŸ› ï¸ Installation and Usage

You need a prepared Python environment and a browser supporting Tampermonkey scripts (e.g., Chrome, Firefox, Edge).

### 1. Preparation

*   **Install Python Dependencies**
    Open a terminal, navigate to the project root directory, and run the following command:
    ```bash
    pip install -r requirements.txt
    ```

*   **Install Tampermonkey Script Manager**
    Install the [Tampermonkey](https://www.tampermonkey.net/) extension for your browser.

*   **Install the Project's Tampermonkey Script**
    1.  Open the Tampermonkey extension management panel.
    2.  Click "Add New Script" or "Create a new script".
    3.  Copy and paste all the code from [`TampermonkeyScript/LMArenaApiBridge.js`](TampermonkeyScript/LMArenaApiBridge.js) file into the editor.
    4.  Save the script.

### 2. Run the Main Program

1.  **Start the Local Server**
    In the project root directory, run the main service program:
    ```bash
    python api_server.py
    ```
    When you see the server start prompt at `http://127.0.0.1:5102`, the server is ready.

2.  **Keep the LMArena Page Open**
    Ensure you have at least one LMArena page open, and the Tampermonkey script has successfully connected to the local server (the page title will start with `âœ…`). You don't need to stay on the conversation page; any page under the domain will work.

### 3. Update Available Model List (Optional but Recommended)
This step generates the `available_models.json` file, letting you know which models are currently available on LMArena, making it easier to update `models.json`.
1.  **Ensure the main server is running**.
2.  Open **a new terminal**, and run the model updater:
    ```bash
    python model_updater.py
    ```
3.  The script will automatically request the browser to fetch the model list and generate the `available_models.json` file in the root directory.
4.  Open `available_models.json`, find the models you want, and copy their `"publicName"` and `"id"` key-value pairs into the `models.json` file (format: `"publicName": "id"`).

### 4. Configure Session ID (Usually Done Once Unless Switching Models or Original Session Expires)

This is the **most important** step. You need to obtain a valid session ID and message ID for the program to communicate correctly with the LMArena API.

1.  **Ensure the main server is running**
    `api_server.py` must be running because the ID updater needs it to activate the browser's capture function.

2.  **Run the ID Updater**
    Open **a new terminal**, navigate to the project root directory, and run the `id_updater.py` script:
    ```bash
    python id_updater.py
    ```
    *   After selection, it will notify the running main server.

3.  **Activate and Capture**
    *   At this point, you should see a crosshair icon (ğŸ¯) appear at the beginning of the LMArena page's title bar, indicating that **ID capture mode is active**.
    *   In the browser, open a LMArena arena page for the **target model's message**. Note that if it's a Battle page, do not view the model name, remain anonymous, and ensure the last message in the current interface is a response from the target model; if it's Direct Chat, ensure the last message in the current interface is a response from the target model.
    *   **Click the Retry button on the target model's response card**.
    *   The Tampermonkey script will capture the `sessionId` and `messageId` and send them to `id_updater.py`.

4.  **Verify the Results**
    *   Return to the terminal where you ran `id_updater.py`, and you will see it print the successfully captured IDs and indicate that they have been written to the `config.jsonc` file.
    *   The script will automatically close upon success. Your configuration is now complete!

### 5. Configure Your OpenAI Client
Point your client or application's OpenAI API address to the local server:
*   **API Base URL**: `http://127.0.0.1:5102/v1`
*   **API Key**: If the `api_key` in `config.jsonc` is empty, you can enter anything; if set, you must provide the correct Key.
*   **Model Name**: Specify the model name you want to use in your client (must match the name in `models.json` exactly). The server will look up the corresponding model ID based on this name.

### 6. Start Chatting! ğŸ’¬
You can now use your client normally, and all requests will be proxied to LMArena through the local server!

## ğŸ¤” How Does It Work?

This project consists of two parts: a local Python **FastAPI** server and a **Tampermonkey script** running in the browser. They work together through **WebSocket**.

```mermaid
sequenceDiagram
    participant C as OpenAI Client ğŸ’»
    participant S as Local FastAPI Server ğŸ
    participant MU as Model Updater Script (model_updater.py) ğŸ“‹
    participant IU as ID Updater Script (id_updater.py) ğŸ†”
    participant T as Tampermonkey Script ğŸµ (in LMArena Page)
    participant L as LMArena.ai ğŸŒ

    alt Initialization
        T->>+S: (Page Load) Establish WebSocket Connection
        S-->>-T: Connection Confirmation
    end

    alt Manual Model List Update (Optional)
        MU->>+S: (User Runs) POST /internal/request_model_update
        S->>T: (WebSocket) Send 'send_page_source' Command
        T->>T: Fetch Page HTML
        T->>S: (HTTP) POST /internal/update_available_models (with HTML)
        S->>S: Parse HTML and Save to available_models.json
        S-->>-MU: Confirmation
    end

    alt Manual Session ID Update
        IU->>+S: (User Runs) POST /internal/start_id_capture
        S->>T: (WebSocket) Send 'activate_id_capture' Command
        T->>L: (User Clicks Retry) Intercept fetch Request
        T->>IU: (HTTP) Send Captured ID
        IU->>IU: Update config.jsonc
        IU-->>-T: Confirmation
    end

    alt Normal Chat Flow
        C->>+S: (User Chat) /v1/chat/completions Request
        S->>S: Convert Request to LMArena Format (and Get Model ID from models.json)
        S->>T: (WebSocket) Send Message with request_id and Payload
        T->>L: (fetch) Send Real Request to LMArena API
        L-->>T: (Streaming) Return Model Response
        T->>S: (WebSocket) Send Response Data Chunks Back
        S-->>-C: (Streaming) Return OpenAI Format Response
    end

    alt Normal Chat Flow (with Image Generation)
        C->>+S: (User Chat) /v1/chat/completions Request
        S->>S: Check Model Name
        alt If Image Generation Model (e.g., DALL-E)
            S->>S: (Parallel) Create n Image Generation Tasks
            S->>T: (WebSocket) Send n Tasks with request_id
            T->>L: (fetch) Send n Real Requests
            L-->>T: (Streaming) Return Image URLs
            T->>S: (WebSocket) Send URLs Back
            S->>S: Format URLs as Markdown Text
            S-->>-C: (HTTP) Return Chat Response with Markdown Images
        else If Standard Text Model
            S->>S: Convert Request to LMArena Format
            S->>T: (WebSocket) Send Message with request_id and Payload
            T->>L: (fetch) Send Real Request to LMArena API
            L-->>T: (Streaming) Return Model Response
            T->>S: (WebSocket) Send Response Data Chunks Back
            S-->>-C: (Streaming) Return OpenAI Format Response
        end
    end
```

1.  **Establish Connection**: When you open the LMArena page in the browser, the **Tampermonkey script** immediately establishes a persistent **WebSocket connection** with the **local FastAPI server**.
    > **Note**: The current architecture assumes only one browser tab is working. If multiple pages are opened, only the last connection will be active.
2.  **Receive Requests**: The **OpenAI client** sends standard chat requests to the local server, specifying the `model` name in the request body.
3.  **Task Distribution**: Upon receiving a request, the server looks up the corresponding model ID from `models.json` based on the `model` name, converts the request to the LMArena-required format, attaches a unique request ID (`request_id`), and sends this task to the connected Tampermonkey script via WebSocket.
4.  **Execution and Response**: The Tampermonkey script, upon receiving the task, directly makes a `fetch` request to the LMArena API endpoint. As LMArena returns a streaming response, the Tampermonkey script captures these data chunks and sends them back to the local server through WebSocket.
5.  **Response Relay**: The server, based on the `request_id` attached to each data chunk, places them in the correct response queue and streams this data back to the OpenAI client in real-time.

## ğŸ“– API Endpoints

### Get Model List

*   **Endpoint**: `GET /v1/models`
*   **Description**: Returns a model list compatible with OpenAI, read from the `models.json` file.

### Chat Completion

*   **Endpoint**: `POST /v1/chat/completions`
*   **Description**: Receives standard OpenAI chat requests, supporting both streaming and non-streaming responses.

### Image Generation (Integrated)

*   **Endpoint**: `POST /v1/chat/completions`
*   **Description**: The image generation feature is now fully integrated into the main chat endpoint. To generate images, simply specify an image model (e.g., `"model": "dall-e-3"`) in the request body and send the request like a normal chat message. The server will automatically recognize and process it.
*   **Request Example**:
    ```bash
    curl http://127.0.0.1:5102/v1/chat/completions \
      -H "Content-Type: application/json" \
      -d '{
        "model": "dall-e-3",
        "messages": [
          {
            "role": "user",
            "content": "A futuristic cityscape at sunset, neon lights, flying cars"
          }
        ],
        "n": 1
      }'
    ```
*   **Response Example (Consistent with Normal Chat)**:
    ```json
    {
      "id": "img-as-chat-...",
      "object": "chat.completion",
      "created": 1677663338,
      "model": "dall-e-3",
      "choices": [
        {
          "index": 0,
          "message": {
            "role": "assistant",
            "content": "![A futuristic cityscape at sunset, neon lights, flying cars](https://...)"
          },
          "finish_reason": "stop"
        }
      ],
      "usage": { ... }
    }
    ```

## ğŸ“‚ File Structure

```
.
â”œâ”€â”€ .gitignore                  # Git å¿½ç•¥æ–‡ä»¶
â”œâ”€â”€ api_server.py               # æ ¸å¿ƒåç«¯æœåŠ¡ (FastAPI) ğŸ
â”œâ”€â”€ id_updater.py               # ä¸€é”®å¼ä¼šè¯IDæ›´æ–°è„šæœ¬ ğŸ†”
â”œâ”€â”€ model_updater.py              # æ‰‹åŠ¨æ¨¡å‹åˆ—è¡¨æ›´æ–°è„šæœ¬ ğŸ“‹
â”œâ”€â”€ models.json                 # æ ¸å¿ƒæ¨¡å‹æ˜ å°„è¡¨ (éœ€æ‰‹åŠ¨ç»´æŠ¤) ğŸ—ºï¸
â”œâ”€â”€ available_models.json       # å¯ç”¨æ¨¡å‹å‚è€ƒåˆ—è¡¨ (è‡ªåŠ¨ç”Ÿæˆ) ğŸ“„
â”œâ”€â”€ model_endpoint_map.json     # [é«˜çº§] æ¨¡å‹åˆ°ä¸“å±ä¼šè¯IDçš„æ˜ å°„è¡¨ ğŸ¯
â”œâ”€â”€ requirements.txt            # Python ä¾èµ–åŒ…åˆ—è¡¨ ğŸ“¦
â”œâ”€â”€ README.md                   # å°±æ˜¯ä½ ç°åœ¨æ­£åœ¨çœ‹çš„è¿™ä¸ªæ–‡ä»¶ ğŸ‘‹
â”œâ”€â”€ config.jsonc                # å…¨å±€åŠŸèƒ½é…ç½®æ–‡ä»¶ âš™ï¸
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ update_script.py        # è‡ªåŠ¨æ›´æ–°é€»è¾‘è„šæœ¬ ğŸ”„
â””â”€â”€ TampermonkeyScript/
    â””â”€â”€ LMArenaApiBridge.js     # å‰ç«¯è‡ªåŠ¨åŒ–æ²¹çŒ´è„šæœ¬ ğŸµ
```

**Enjoy the freedom to explore the world of models in LMArena!** ğŸ’–
